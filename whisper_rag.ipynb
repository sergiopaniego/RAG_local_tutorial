{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sergiopaniego/RAG_local_tutorial/blob/main/whisper_rag.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG example with an audio\n",
    "\n",
    "In this example, we use an audio as information source for the conversation. \n",
    "\n",
    "**Please, complete the example_rag.ipynb first to get more insight.**\n",
    "\n",
    "Imagine a situation where you directly communicate through voice with the LLM.\n",
    "\n",
    "In this case, we use Whisper, a general-purpose speech recognition model by OpenAI to convert an audio to text.\n",
    "\n",
    "From that text, we can generate a conversation with the LLM model to extract the required information.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2023/07/langchain3.png\" alt=\"Langchain Logo\" width=\"20%\">\n",
    "  <img src=\"https://bookface-images.s3.amazonaws.com/logos/ee60f430e8cb6ae769306860a9c03b2672e0eaf2.png\" alt=\"Ollama Logo\" width=\"20%\">\n",
    "  <img src=\"https://static-00.iconduck.com/assets.00/openai-icon-2021x2048-4rpe5x7n.png\" alt=\"OpenAI Logo\" width=\"20%\">\n",
    "</p>\n",
    "\n",
    "\n",
    "Sources:\n",
    "\n",
    "* Whisper details: https://github.com/openai/whisper\n",
    "* https://github.com/svpino/llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we install Whisper and its dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install openai-whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We install ffmpeg version depending on the platform where we'll be running the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on Ubuntu or Debian\n",
    "#sudo apt update && sudo apt install ffmpeg\n",
    "\n",
    "# on Arch Linux\n",
    "#sudo pacman -S ffmpeg\n",
    "\n",
    "# on MacOS using Homebrew (https://brew.sh/)\n",
    "!brew install ffmpeg\n",
    "\n",
    "# on Windows using Chocolatey (https://chocolatey.org/)\n",
    "#choco install ffmpeg\n",
    "\n",
    "# on Windows using Scoop (https://scoop.sh/)\n",
    "#scoop install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given an example audio, we transcribe it to text so we can use it.\n",
    "\n",
    "The example audio should be downloaded locally. Change the file name with your own example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    result = whisper_model.transcribe(audio_path)\n",
    "    return result['text']\n",
    "\n",
    "# https://commons.wikimedia.org/wiki/File:Audio_Kevin_Folta.wav\n",
    "# Audio example\n",
    "audio_path = \"./files/Audio_Kevin_Folta.wav\" # CHANGE THIS FILE\n",
    "\n",
    "# Transcribir el audio\n",
    "transcribed_text = transcribe_audio(audio_path)\n",
    "\n",
    "print(transcribed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The rest of the code is essentially the same as in the first RAG example\n",
    "\n",
    "We instantiate the model, we then generate the PromptTemplate where the transcribed audio is used as context and we can ask questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"llama3\"\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "model = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    print(\"Say 'exit' or 'quit' to exit the loop\")\n",
    "    question = input('User question: ')\n",
    "    print(f\"Question: {question}\")\n",
    "    if question.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting the conversation. Goodbye!\")\n",
    "        break\n",
    "    formatted_prompt = prompt.format(context=transcribed_text, question=question)\n",
    "    response_from_model = model.invoke(formatted_prompt)\n",
    "    parsed_response = parser.parse(response_from_model)\n",
    "    print(f\"Answer: {parsed_response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

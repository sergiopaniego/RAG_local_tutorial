{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sergiopaniego/RAG_local_tutorial/blob/main/whisper_rag.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG example with an audio\n",
    "\n",
    "In this example, we use an audio as information source for the conversation. \n",
    "\n",
    "**Please, complete the example_rag.ipynb first to get more insight.**\n",
    "\n",
    "Imagine a situation where you directly communicate through voice with the LLM.\n",
    "\n",
    "In this case, we use Whisper, a general-purpose speech recognition model by OpenAI to convert an audio to text.\n",
    "\n",
    "From that text, we can generate a conversation with the LLM model to extract the required information.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2023/07/langchain3.png\" alt=\"Langchain Logo\" width=\"20%\">\n",
    "  <img src=\"https://bookface-images.s3.amazonaws.com/logos/ee60f430e8cb6ae769306860a9c03b2672e0eaf2.png\" alt=\"Ollama Logo\" width=\"20%\">\n",
    "  <img src=\"https://static-00.iconduck.com/assets.00/openai-icon-2021x2048-4rpe5x7n.png\" alt=\"OpenAI Logo\" width=\"20%\">\n",
    "</p>\n",
    "\n",
    "\n",
    "Sources:\n",
    "\n",
    "* Whisper details: https://github.com/openai/whisper\n",
    "* https://github.com/svpino/llm\n",
    "\n",
    "# Requirements\n",
    "\n",
    "* Ollama installed locally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we install Whisper and its dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install langchain\n",
    "!pip3 install langchain_pinecone\n",
    "!pip3 install langchain[docarray]\n",
    "!pip3 install docarray\n",
    "!pip3 install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install openai-whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We install ffmpeg version depending on the platform where we'll be running the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on Ubuntu or Debian\n",
    "#sudo apt update && sudo apt install ffmpeg\n",
    "\n",
    "# on Arch Linux\n",
    "#sudo pacman -S ffmpeg\n",
    "\n",
    "# on MacOS using Homebrew (https://brew.sh/)\n",
    "!brew install ffmpeg\n",
    "\n",
    "# on Windows using Chocolatey (https://chocolatey.org/)\n",
    "#choco install ffmpeg\n",
    "\n",
    "# on Windows using Scoop (https://scoop.sh/)\n",
    "#scoop install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the LLM model to use\n",
    "\n",
    "The model must be downloaded locally to be used, so if you want to run llama3, you should run:\n",
    "\n",
    "```\n",
    "\n",
    "ollama pull llama3\n",
    "\n",
    "```\n",
    "\n",
    "Check the list of models available for Ollama here: https://ollama.com/library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given an example audio, we transcribe it to text so we can use it.\n",
    "\n",
    "The example audio should be downloaded locally. Change the file name with your own example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergiopaniegoblanco/Library/Python/3.9/lib/python/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi, my name is Kevin Fowler and I'm very grateful to have the opportunity to work in public science. I'm very fortunate to have been able to be trained by excellent mentors early in my career to give me a toolbox to be able to unravel important questions that can help us better understand our physical universe. I'm really excited about the opportunity to participate in the science that works around agriculture and find out ways that we can farm more sustainably, working in ways that we can help limit environmental impacts of farming, but also providing profitable ways for farmers to stay in business. We're really excited about the technologies that we create creating better foods for the American consumer and industrialized world consumer, but also getting to people in need so that they can have more selection of better fruits and vegetables. Going forward, I think that the future will depend upon the integration of all technologies at the table and it's really important that we pay attention to this new breakthroughs and integrate them into our agricultural schemes. The future of agriculture in medicine room really relies on us being open to all new solutions and using all of them to solve our problems to have better planet and better people.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    result = whisper_model.transcribe(audio_path)\n",
    "    return result['text']\n",
    "\n",
    "# https://commons.wikimedia.org/wiki/File:Audio_Kevin_Folta.wav\n",
    "# Audio example\n",
    "audio_path = \"./files/Audio_Kevin_Folta.wav\" # CHANGE THIS FILE\n",
    "\n",
    "# Transcribir el audio\n",
    "transcribed_text = transcribe_audio(audio_path)\n",
    "\n",
    "print(transcribed_text)\n",
    "\n",
    "with open(\"./files/whisper_transcription.txt\", \"a\") as file:\n",
    "    file.write(transcribed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We instantiate the model and the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL = \"gpt-3.5-turbo\"\n",
    "#MODEL = \"mixtral:8x7b\"\n",
    "#MODEL = \"gemma:7b\"\n",
    "#MODEL = \"llama2\"\n",
    "MODEL = \"llama3\" # https://ollama.com/library/llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "\n",
    "model = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We load the transcription previously saved using TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\" Hi, my name is Kevin Fowler and I'm very grateful to have the opportunity to work in public science. I'm very fortunate to have been able to be trained by excellent mentors early in my career to give me a toolbox to be able to unravel important questions that can help us better understand our physical universe. I'm really excited about the opportunity to participate in the science that works around agriculture and find out ways that we can farm more sustainably, working in ways that we can help limit environmental impacts of farming, but also providing profitable ways for farmers to stay in business. We're really excited about the technologies that we create creating better foods for the American consumer and industrialized world consumer, but also getting to people in need so that they can have more selection of better fruits and vegetables. Going forward, I think that the future will depend upon the integration of all technologies at the table and it's really important that we pay attention to this new breakthroughs and integrate them into our agricultural schemes. The future of agriculture in medicine room really relies on us being open to all new solutions and using all of them to solve our problems to have better planet and better people.\", metadata={'source': './files/whisper_transcription.txt'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./files/whisper_transcription.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We explit the document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Hi, my name is Kevin Fowler and I'm very grateful to have the opportunity to work in public\", metadata={'source': './files/whisper_transcription.txt'}),\n",
       " Document(page_content=\"to work in public science. I'm very fortunate to have been able to be trained by excellent mentors\", metadata={'source': './files/whisper_transcription.txt'}),\n",
       " Document(page_content='excellent mentors early in my career to give me a toolbox to be able to unravel important questions', metadata={'source': './files/whisper_transcription.txt'}),\n",
       " Document(page_content=\"important questions that can help us better understand our physical universe. I'm really excited\", metadata={'source': './files/whisper_transcription.txt'}),\n",
       " Document(page_content=\"I'm really excited about the opportunity to participate in the science that works around\", metadata={'source': './files/whisper_transcription.txt'})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "text_documents = text_splitter.split_documents(text_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the PDF in a vector space.\n",
    "\n",
    "From Langchain docs:\n",
    "\n",
    "`DocArrayInMemorySearch is a document index provided by Docarray that stores documents in memory. It is a great starting point for small datasets, where you may not want to launch a database server.`\n",
    "\n",
    "The execution time of the following block depends on the complexity and longitude of the PDF provided. Try to keep it small and simple for the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergiopaniegoblanco/Library/Python/3.9/lib/python/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(text_documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the conversation template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We instantiate the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can now extract the information from the audio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say 'exit' or 'quit' to exit the loop\n",
      "Question: What's his name?\n",
      "Answer: Kevin Fowler.\n",
      "\n",
      "Say 'exit' or 'quit' to exit the loop\n",
      "Question: exit\n",
      "Exiting the conversation. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print(\"Say 'exit' or 'quit' to exit the loop\")\n",
    "    question = input('User question: ')\n",
    "    print(f\"Question: {question}\")\n",
    "    if question.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting the conversation. Goodbye!\")\n",
    "        break\n",
    "    retrieved_context = retriever.invoke(question)\n",
    "    formatted_prompt = prompt.format(context=retrieved_context, question=question)\n",
    "    response_from_model = model.invoke(formatted_prompt)\n",
    "    parsed_response = parser.parse(response_from_model)\n",
    "    print(f\"Answer: {parsed_response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

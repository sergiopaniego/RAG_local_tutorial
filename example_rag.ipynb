{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sergiopaniego/RAG_local_tutorial/blob/main/example.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG example with Langchain, Ollama and and open-source LLM model\n",
    "\n",
    "Sources:\n",
    "\n",
    "* https://github.com/svpino/llm\n",
    "* https://github.com/AIAnytime/Gemma-7B-RAG-using-Ollama/blob/main/Ollama%20Gemma.ipynb\n",
    "* https://www.youtube.com/watch?v=-MexTC18h20&ab_channel=AIAnytime\n",
    "* https://www.youtube.com/watch?v=HRvyei7vFSM&ab_channel=Underfitted\n",
    "\n",
    " \n",
    "# Requirements\n",
    "\n",
    "* Ollama installed locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the requirements\n",
    "\n",
    "If an error is raised related to docarray, refer to this solution: https://stackoverflow.com/questions/76880224/error-using-using-docarrayinmemorysearch-in-langchain-could-not-import-docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install langchain\n",
    "!pip3 install langchain_pinecone\n",
    "!pip3 install langchain[docarray]\n",
    "!pip3 install docarray\n",
    "!pip3 install pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the LLM model to use\n",
    "\n",
    "The model must be downloaded locally to be used, so if you want to run llama3, you should run:\n",
    "\n",
    "```\n",
    "\n",
    "ollama pull llama3\n",
    "\n",
    "```\n",
    "\n",
    "Check the list of models available for Ollama here: https://ollama.com/library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL = \"gpt-3.5-turbo\"\n",
    "#MODEL = \"mixtral:8x7b\"\n",
    "#MODEL = \"gemma:7b\"\n",
    "#MODEL = \"llama2\"\n",
    "MODEL = \"llama3\" # https://ollama.com/library/llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We instanciate the LLM model and the Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s one that I hope will inspire you:\\n\\n\"Believe you can and you\\'re halfway there.\" - Theodore Roosevelt\\n\\nI hope this quote gives you a boost of motivation and confidence to tackle your goals and challenges!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "model = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)\n",
    "\n",
    "model.invoke(\"Give me an inspirational quote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer to 2+2 is 4.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Waht is 2+2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a parser provided by LangChain, we can transform the LLM output to something more suitable to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "\"Believe you can and you're halfway there.\" - Theodore Roosevelt\n",
      "\n",
      "I hope it inspires you to tackle your goals and pursue your passions!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "response_from_model = model.invoke(\"Give me an inspirational quote\")\n",
    "parsed_response = parser.parse(response_from_model)\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We generate the template for the conversation with the instruct-based LLM\n",
    "\n",
    "We can create a template to structure the conversation effectively.\n",
    "\n",
    "This template allows us to provide some general context to the Language Learning Model (LLM), which will be utilized for every prompt. This ensures that the model has a consistent background understanding for all interactions.\n",
    "\n",
    "Additionally, we can include specific context relevant to the particular prompt. This helps the model understand the immediate scenario or topic before addressing the actual question. Following this specific context, we then present the actual question we want the model to answer.\n",
    "\n",
    "By using this approach, we enhance the model's ability to generate accurate and relevant responses based on both the general and specific contexts provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, answer with \"I don\\'t know\".\\n\\nContext: Here is some context\\n\\nQuestion: Here is a question\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Here is some context\", question=\"Here is a question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can answer prompts based on the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Sergio!\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = prompt.format(context=\"My parents named me Sergio\", question=\"What's your name?\")\n",
    "response_from_model = model.invoke(formatted_prompt)\n",
    "parsed_response = parser.parse(response_from_model)\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can't answer what is not provided as context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know!\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = prompt.format(context=\"My parents named me Sergio\", question=\"What's my age?\")\n",
    "response_from_model = model.invoke(formatted_prompt)\n",
    "parsed_response = parser.parse(response_from_model)\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even previously known info!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know!\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = prompt.format(context=\"My parents named me Sergio\", question=\"What is 2+2?\")\n",
    "response_from_model = model.invoke(formatted_prompt)\n",
    "parsed_response = parser.parse(response_from_model)\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load an example PDF to do Retrieval Augmented Generation (RAG)\n",
    "\n",
    "For the example, you can select your own PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Ser gio Paniego\\nI’m a PhD candidate at Univ ersidad Re y Juan Carlos , Madrid, r esear ching ar ti\\x00cial intelligence,\\ncomputer vision and r obotics and its application t o aut onomous driving. I obtained an\\nInformatics Engineering and Softwar e Engineering Dual Degr ee at Univ ersidad Re y Juan\\nCarlos and a MSc in Ar ti\\x00cial Intelligence at Univ ersidad P olitécnica de Madrid with a José\\nCuena awar d.\\nThe pr ojects I’ m working on now can be found in my GitHub account .\\nIn addition t o resear ch, I’ m also a teaching assistant for some courses (Ar ti\\x00cial Intelligence\\nand Robotics) for thir d-year students at the same uni r elated t o my r esear ch inter ests. I ha ve\\nalso de veloped a course for Platzi  platform called Object Detection and Segmentation with\\nTensor\\x00ow  among others ( teaching section ). I also lik e to giv e talks  about my work and\\narti\\x00cial intelligence in gener al.\\nI have par ticipated in Google Summer of Code  program six y ears in a r ow. The \\x00rst one was in\\n2018  as a Student Intern working with JdeRobot Robotics and Ar ti\\x00cial Intelligence\\nassociation  and the following \\x00v e as a ment or in se veral ar ti\\x00cial intelligence r elated pr ojects,\\nDetection Studio , Beha vior Studio  and Deep Learning Studio  with the same association, wher e\\nI still collabor ate. These pr ojects and others ha ve led t o publications .\\nFor mor e information about my pr ofessional car eer visit the cv section  or my LinkedIn page , wher e I post news.\\nI am open t o resear ch collabor ations, teaching courses, talks… Drop me an email  to get in t ouch :)\\nOne of the hobbies that I enjo y the most is phot ography .\\n\\uf0e0 \\ue9d4 \\uf09b \\uf08c \\uf099\\nDrop me an email to get in touch :)\\n© Cop yright 2024 Ser gio P aniego. P ower ed b y Jekyll  with al-folio  theme. Hosted b y GitHub P ages .25/4/24, 17:27 Sergio Paniego\\nhttps://sergiopaniego.github.io 1/1', metadata={'source': 'example.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"example.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the PDF in a vector space.\n",
    "\n",
    "From Langchain docs:\n",
    "\n",
    "`DocArrayInMemorySearch is a document index provided by Docarray that stores documents in memory. It is a great starting point for small datasets, where you may not want to launch a database server.`\n",
    "\n",
    "The execution time of the following block depends on the complexity and longitude of the PDF provided. Try to keep it small and simple for the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergiopaniegoblanco/Library/Python/3.9/lib/python/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create retriever of vectors that are similar to be used as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Ser gio Paniego\\nI’m a PhD candidate at Univ ersidad Re y Juan Carlos , Madrid, r esear ching ar ti\\x00cial intelligence,\\ncomputer vision and r obotics and its application t o aut onomous driving. I obtained an\\nInformatics Engineering and Softwar e Engineering Dual Degr ee at Univ ersidad Re y Juan\\nCarlos and a MSc in Ar ti\\x00cial Intelligence at Univ ersidad P olitécnica de Madrid with a José\\nCuena awar d.\\nThe pr ojects I’ m working on now can be found in my GitHub account .\\nIn addition t o resear ch, I’ m also a teaching assistant for some courses (Ar ti\\x00cial Intelligence\\nand Robotics) for thir d-year students at the same uni r elated t o my r esear ch inter ests. I ha ve\\nalso de veloped a course for Platzi  platform called Object Detection and Segmentation with\\nTensor\\x00ow  among others ( teaching section ). I also lik e to giv e talks  about my work and\\narti\\x00cial intelligence in gener al.\\nI have par ticipated in Google Summer of Code  program six y ears in a r ow. The \\x00rst one was in\\n2018  as a Student Intern working with JdeRobot Robotics and Ar ti\\x00cial Intelligence\\nassociation  and the following \\x00v e as a ment or in se veral ar ti\\x00cial intelligence r elated pr ojects,\\nDetection Studio , Beha vior Studio  and Deep Learning Studio  with the same association, wher e\\nI still collabor ate. These pr ojects and others ha ve led t o publications .\\nFor mor e information about my pr ofessional car eer visit the cv section  or my LinkedIn page , wher e I post news.\\nI am open t o resear ch collabor ations, teaching courses, talks… Drop me an email  to get in t ouch :)\\nOne of the hobbies that I enjo y the most is phot ography .\\n\\uf0e0 \\ue9d4 \\uf09b \\uf08c \\uf099\\nDrop me an email to get in touch :)\\n© Cop yright 2024 Ser gio P aniego. P ower ed b y Jekyll  with al-folio  theme. Hosted b y GitHub P ages .25/4/24, 17:27 Sergio Paniego\\nhttps://sergiopaniego.github.io 1/1', metadata={'source': 'example.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"artificial intelligence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate conversate with the document to extract the details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming retriever is an instance of a retriever class and has a method to retrieve context\n",
    "retrieved_context = retriever.invoke(\"artificial intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are his research interests?\n",
      "Answer: Based on the context, Ser gio Paniego's research interests include artificial intelligence, computer vision, and robotics, with a focus on autonomous driving.\n",
      "\n",
      "Question: Does he have teaching experience?\n",
      "Answer: Yes, according to the context, Ser gio Paniego has teaching experience as a teaching assistant for some courses (Ar ti\\x00cial Intelligence and Robotics) for third-year students at Univ ersidad Re y Juan Carlos. He also developed a course for Platzi platform called Object Detection and Segmentation with Tensor\\x00ow among others.\n",
      "\n",
      "Question: What are his hobbies?\n",
      "Answer: According to the context, one of Ser gio Paniego's hobbies is photography.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What are his research interests?\",\n",
    "    \"Does he have teaching experience?\",\n",
    "    \"What are his hobbies?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    formatted_prompt = prompt.format(context=retrieved_context, question=question)\n",
    "    response_from_model = model.invoke(formatted_prompt)\n",
    "    parsed_response = parser.parse(response_from_model)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {parsed_response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop to ask-answer questions continously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say 'exit' or 'quit' to exit the loop\n",
      "Question: Does he kwon about deep learning?\n",
      "Answer: Yes. According to the context, Ser gio Paniego has worked on projects related to artificial intelligence, computer vision, and robotics, including Deep Learning Studio with JdeRobot Robotics and Artificial Intelligence association. He also mentions having a course for Platzi platform called Object Detection and Segmentation with TensorFlow.\n",
      "\n",
      "Say 'exit' or 'quit' to exit the loop\n",
      "Question: exit\n",
      "Exiting the conversation. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print(\"Say 'exit' or 'quit' to exit the loop\")\n",
    "    question = input('User question: ')\n",
    "    print(f\"Question: {question}\")\n",
    "    if question.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting the conversation. Goodbye!\")\n",
    "        break\n",
    "    formatted_prompt = prompt.format(context=retrieved_context, question=question)\n",
    "    response_from_model = model.invoke(formatted_prompt)\n",
    "    parsed_response = parser.parse(response_from_model)\n",
    "    print(f\"Answer: {parsed_response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

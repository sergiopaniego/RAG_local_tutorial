{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sergiopaniego/RAG_local_tutorial/blob/main/example_rag.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG example with Langchain, Ollama and and open-source LLM model\n",
    "\n",
    "Sources:\n",
    "\n",
    "* https://github.com/svpino/llm\n",
    "* https://github.com/AIAnytime/Gemma-7B-RAG-using-Ollama/blob/main/Ollama%20Gemma.ipynb\n",
    "* https://www.youtube.com/watch?v=-MexTC18h20&ab_channel=AIAnytime\n",
    "* https://www.youtube.com/watch?v=HRvyei7vFSM&ab_channel=Underfitted\n",
    "\n",
    " \n",
    "# Requirements\n",
    "\n",
    "* Ollama installed locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the requirements\n",
    "\n",
    "If an error is raised related to docarray, refer to this solution: https://stackoverflow.com/questions/76880224/error-using-using-docarrayinmemorysearch-in-langchain-could-not-import-docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install langchain\n",
    "!pip3 install langchain_pinecone\n",
    "!pip3 install langchain[docarray]\n",
    "!pip3 install docarray\n",
    "!pip3 install pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the LLM model to use\n",
    "\n",
    "The model must be downloaded locally to be used, so if you want to run llama3, you should run:\n",
    "\n",
    "```\n",
    "\n",
    "ollama pull llama3\n",
    "\n",
    "```\n",
    "\n",
    "Check the list of models available for Ollama here: https://ollama.com/library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL = \"gpt-3.5-turbo\"\n",
    "#MODEL = \"mixtral:8x7b\"\n",
    "#MODEL = \"gemma:7b\"\n",
    "#MODEL = \"llama2\"\n",
    "MODEL = \"llama3\" # https://ollama.com/library/llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We instanciate the LLM model and the Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "model = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)\n",
    "\n",
    "model.invoke(\"Give me an inspirational quote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\"Waht is 2+2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a parser provided by LangChain, we can transform the LLM output to something more suitable to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "response_from_model = model.invoke(\"Give me an inspirational quote\")\n",
    "parsed_response = parser.parse(response_from_model)\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We generate the template for the conversation with the instruct-based LLM\n",
    "\n",
    "We can create a template to structure the conversation effectively.\n",
    "\n",
    "This template allows us to provide some general context to the Language Learning Model (LLM), which will be utilized for every prompt. This ensures that the model has a consistent background understanding for all interactions.\n",
    "\n",
    "Additionally, we can include specific context relevant to the particular prompt. This helps the model understand the immediate scenario or topic before addressing the actual question. Following this specific context, we then present the actual question we want the model to answer.\n",
    "\n",
    "By using this approach, we enhance the model's ability to generate accurate and relevant responses based on both the general and specific contexts provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Here is some context\", question=\"Here is a question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can answer prompts based on the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = prompt.format(context=\"My parents named me Sergio\", question=\"What's your name?\")\n",
    "response_from_model = model.invoke(formatted_prompt)\n",
    "parsed_response = parser.parse(response_from_model)\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can't answer what is not provided as context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = prompt.format(context=\"My parents named me Sergio\", question=\"What's my age?\")\n",
    "response_from_model = model.invoke(formatted_prompt)\n",
    "parsed_response = parser.parse(response_from_model)\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even previously known info!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = prompt.format(context=\"My parents named me Sergio\", question=\"What is 2+2?\")\n",
    "response_from_model = model.invoke(formatted_prompt)\n",
    "parsed_response = parser.parse(response_from_model)\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load an example PDF to do Retrieval Augmented Generation (RAG)\n",
    "\n",
    "For the example, you can select your own PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"./files/example.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the PDF in a vector space.\n",
    "\n",
    "From Langchain docs:\n",
    "\n",
    "`DocArrayInMemorySearch is a document index provided by Docarray that stores documents in memory. It is a great starting point for small datasets, where you may not want to launch a database server.`\n",
    "\n",
    "The execution time of the following block depends on the complexity and longitude of the PDF provided. Try to keep it small and simple for the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create retriever of vectors that are similar to be used as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"artificial intelligence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate conversate with the document to extract the details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming retriever is an instance of a retriever class and has a method to retrieve context\n",
    "retrieved_context = retriever.invoke(\"artificial intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What are his research interests?\",\n",
    "    \"Does he have teaching experience?\",\n",
    "    \"What are his hobbies?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    formatted_prompt = prompt.format(context=retrieved_context, question=question)\n",
    "    response_from_model = model.invoke(formatted_prompt)\n",
    "    parsed_response = parser.parse(response_from_model)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {parsed_response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop to ask-answer questions continously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    print(\"Say 'exit' or 'quit' to exit the loop\")\n",
    "    question = input('User question: ')\n",
    "    print(f\"Question: {question}\")\n",
    "    if question.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting the conversation. Goodbye!\")\n",
    "        break\n",
    "    formatted_prompt = prompt.format(context=retrieved_context, question=question)\n",
    "    response_from_model = model.invoke(formatted_prompt)\n",
    "    parsed_response = parser.parse(response_from_model)\n",
    "    print(f\"Answer: {parsed_response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

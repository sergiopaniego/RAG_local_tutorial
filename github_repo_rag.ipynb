{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lightning.ai/lightning-ai/studios/chat-with-your-code-using-rag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install llama_index\n",
    "!pip3 install llama-index-readers-github\n",
    "!pip3 install llama-index-embeddings-langchain\n",
    "!pip3 install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is due to the fact that we use asyncio.loop_until_complete in\n",
    "# the DiscordReader. Since the Jupyter kernel itself runs on\n",
    "# an event loop, we need to add some help with nesting\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_ACCESS_TOKEN=\"GITHUB_API_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.github import GithubRepositoryReader, GithubClient\n",
    "\n",
    "def initialize_github_client(github_token):\n",
    "    return GithubClient(github_token)\n",
    "\n",
    "github_client = initialize_github_client(GITHUB_ACCESS_TOKEN)\n",
    "\n",
    "loader = GithubRepositoryReader(\n",
    "            github_client,\n",
    "            owner='sergiopaniego',\n",
    "            repo='RAG_local_tutorial',\n",
    "            filter_file_extensions=(\n",
    "                [\".ipynb\"],\n",
    "                GithubRepositoryReader.FilterType.INCLUDE,\n",
    "            ),\n",
    "            verbose=False,\n",
    "            concurrent_requests=5,\n",
    "        )\n",
    "\n",
    "docs = loader.load_data(branch=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5')\n",
    "embed_model = LangchainEmbedding(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# ====== Create vector store and upload indexed data ======\n",
    "Settings.embed_model = embed_model # we specify the embedding model to be used\n",
    "index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# setting up the llm\n",
    "llm = Ollama(model=\"llama3\", request_timeout=500.0) \n",
    "\n",
    "# ====== Setup a query engine on the index previously created ======\n",
    "Settings.llm = llm # specifying the llm to be used\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts.base import PromptTemplate\n",
    "\n",
    "\n",
    "qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information above I want you to think step by step to answer the query in a crisp manner, incase case you don't know the answer say 'I don't know!'.\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "            )\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})\n",
    "\n",
    "response = query_engine.query('What is this repository about?')\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

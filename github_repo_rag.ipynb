{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of RAG with a GitHub repo code\n",
    "\n",
    "In this example, we implement a RAG system where you can chat with your GitHub repository code.\n",
    "\n",
    "**Please, complete the example_rag.ipynb first to get more insight.**\n",
    "\n",
    "Let's go!\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://images.contentstack.io/v3/assets/bltac01ee6daa3a1e14/blt45d9c451c9a70269/6542d10b8b3f8e001b7aeead/img_blog_image_inline.png?width=1120&disable=upscale&auto=webp\" alt=\"LlamaIndex Logo\" width=\"20%\">\n",
    "  <img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2023/07/langchain3.png\" alt=\"Langchain Logo\" width=\"20%\">\n",
    "  <img src=\"https://bookface-images.s3.amazonaws.com/logos/ee60f430e8cb6ae769306860a9c03b2672e0eaf2.png\" alt=\"Ollama Logo\" width=\"20%\">\n",
    "  <img src=\"https://cdn-icons-png.flaticon.com/256/25/25231.png\" alt=\"GitHub Logo\" width=\"20%\">\n",
    "</p>\n",
    "\n",
    "Sources:\n",
    "\n",
    "* https://lightning.ai/lightning-ai/studios/chat-with-your-code-using-rag\n",
    "\n",
    "# Requirements\n",
    "\n",
    "* Ollama installed locally\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install llama_index\n",
    "!pip3 install llama-index-readers-github\n",
    "!pip3 install llama-index-embeddings-langchain\n",
    "!pip3 install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell prevents the kernel to collapse when downloading the GitHub repo content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is due to the fact that we use asyncio.loop_until_complete in\n",
    "# the DiscordReader. Since the Jupyter kernel itself runs on\n",
    "# an event loop, we need to add some help with nesting\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's download the repo code!\n",
    "\n",
    "In this case, we target this particular repo but you can change the code to target other repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_ACCESS_TOKEN=\"GITHUB_ACCESS_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.github import GithubRepositoryReader, GithubClient\n",
    "\n",
    "def initialize_github_client(github_token):\n",
    "    return GithubClient(github_token)\n",
    "\n",
    "github_client = initialize_github_client(GITHUB_ACCESS_TOKEN)\n",
    "\n",
    "loader = GithubRepositoryReader(\n",
    "            github_client,\n",
    "            owner='sergiopaniego', # CHANGE\n",
    "            repo='RAG_local_tutorial', # CHANGE\n",
    "            filter_file_extensions=(\n",
    "                [\".ipynb\"],\n",
    "                GithubRepositoryReader.FilterType.INCLUDE,\n",
    "            ),\n",
    "            verbose=False,\n",
    "            concurrent_requests=5,\n",
    "        )\n",
    "\n",
    "docs = loader.load_data(branch=\"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5')\n",
    "embed_model = LangchainEmbedding(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the code into VectorStoreIndex using the loaded embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create vector store and upload indexed data\n",
    "Settings.embed_model = embed_model # Set the embedding model to be used\n",
    "index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the LLM model to make the requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Select the LLM model. You can change the model name below.\n",
    "llm = Ollama(model=\"llama3\", request_timeout=500.0) \n",
    "\n",
    "# Generate a query engine from the previosuly created vector store index\n",
    "Settings.llm = llm # Set the LLM model to be used\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's chat with the code! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This repository is likely about using language models, specifically the LLaMA model, to analyze and generate text related to agriculture and sustainability. The code in this repository appears to be focused on creating a query engine that can answer questions about agricultural practices, sustainable farming methods, and the integration of technologies into these systems. The goal seems to be to develop more efficient and environmentally friendly ways to produce food, while also providing better options for consumers and those in need.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.prompts.base import PromptTemplate\n",
    "\n",
    "response = query_engine.query('What is this repository about?')\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
